{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to WaveNet\n",
    "## Jonathan Balaban\n",
    "\n",
    "WaveNet is a powerful new predictive technique that uses multiple Deep Learning (DL) strategies from Computer Vision (CV) and Audio Signal Processing models and applies them to longitudinal (time-series) data. It was created by researchers at London-based artificial intelligence firm [DeepMind](https://deepmind.com), and currently powers [Google Assistant voices](https://assistant.google.com).\n",
    "\n",
    "We'll explore WaveNet and how it works, but first dive into data prep, current high-performance models (as a baseline, Facebook Prophet), and then compare results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">run this full line in bash, as one line:\n",
    "\n",
    "`conda create -n wavenet python=3.7 pandas numpy seaborn matplotlib jupyter keras`\n",
    "\n",
    ">then install Facebook Prophet:\n",
    "\n",
    "`source activate wavenet; conda install -c conda-forge fbprophet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# load Apple stock\n",
    "aapl_raw = pd.read_csv('./AAPL-2000-2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_raw.info()\n",
    "\n",
    "# we need to set date as datetime object and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index\n",
    "aapl_raw.index = pd.to_datetime(aapl_raw.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# check\n",
    "aapl_raw.head()\n",
    "\n",
    "# looks good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Adj Close to AC, get rid of pesky spacing\n",
    "aapl_raw['AC'] = aapl_raw['Adj Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Based on [this article](https://finance.zacks.com/adjusted-closing-price-vs-closing-price-9991.html) and others, let's use Adj Close as our series."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# calculate MACD (a key trading metric, and helpful exogenous variable)\n",
    "aapl_raw['MACD'] = (aapl_raw.AC.ewm(span=12).mean() - aapl_raw.AC.ewm(span=26).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[MACD is often used by investors as a buy/sell indicator](https://www.investopedia.com/terms/m/macd.asp), so it should help track patterns and trend changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good, drop Date and everything but AC and Volume\n",
    "aapl = aapl_raw.drop(columns=['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close'])\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(aapl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note a few things from the EDA above:\n",
    "- No NULLs\n",
    "- Inverse exponential relationship between AC and Volume?\n",
    "- Right skew in price\n",
    "\n",
    "There's an important consideration also: **stock splits!**\n",
    "\n",
    "Apple's stock has split four times since the company went public. The stock split on a 7-for-1 basis on June 9, 2014 and split on a 2-for-1 basis on February 28, 2005, June 21, 2000, and June 16, 1987.\n",
    "\n",
    "Let's filter to the 2014 and 2005 weeks and see what happened:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# first 2005\n",
    "aapl['2005-02-24':'2005-03-02']\n",
    "\n",
    "# looks like Adj Close accounted for the split automatically"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# next 2014\n",
    "aapl['2014-06-06':'2014-06-11']\n",
    "\n",
    "# good again, but essential to check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot price over time\n",
    "aapl.AC.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optional: run ADF to determine unit root\n",
    "import statsmodels.tsa.stattools as ts\n",
    "cadf = ts.adfuller(aapl_raw.AC)\n",
    "\n",
    "print('Augmented Dickey Fuller:')\n",
    "print('Test Statistic =',cadf[0])\n",
    "print('p-value =',cadf[1])\n",
    "print('Critical Values =',cadf[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# FB Prophet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "# pull only close price and set up dataframe for prophet\n",
    "aapl_fb = aapl\n",
    "aapl_fb['ds'] = aapl_fb.index\n",
    "aapl_fb = aapl_fb.rename(columns={'AC': 'y'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "aapl_fb.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# fit model without final thirty periods\n",
    "fbm = Prophet(weekly_seasonality='auto', daily_seasonality=False, seasonality_mode='additive')\n",
    "\n",
    "fbm.fit(aapl_fb.iloc[:-30,:]);\n",
    "\n",
    "# create future dataset\n",
    "future = fbm.make_future_dataframe(periods=30, freq='D')\n",
    "future.tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check endpoint for train\n",
    "aapl_fb.iloc[-30,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "forecast = fbm.predict(future)\n",
    "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "fbm.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred_plot = fbm.plot(forecast)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# calculate MAE\n",
    "\n",
    "diffs = np.absolute(aapl_fb.y[-30:].values - forecast.yhat[-30:].values)\n",
    "\n",
    "diffs.mean() # this is our metric for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# WaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilated Causal Convolution Layers\n",
    "The model architecture we will build is a variant of the [WaveNet model](https://deepmind.com/blog/wavenet-generative-model-raw-audio/), but optimized for sequence data.\n",
    "\n",
    "The **dilated causal convolution layer** is the default building block; Joe Eddy has a fantastic breakdown of [how it works here](https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Conv_Intro.ipynb) and [in his blog here](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/). In a nutshell, these layers (borrowed and modified from CNNs) are designed to learn detailed patterns from recent stock price movements, but also big-picture trends and cycles from months and years ago.\n",
    "\n",
    "![dilated convolutions](./images/WaveNet_dilatedconv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking back a month and year\n",
    "plt.plot(aapl.AC.iloc[-30:], 'r', linewidth=8, markersize=12);\n",
    "plt.plot(aapl.AC.iloc[-365:]);\n",
    "\n",
    "# what intel can we gather from each timeframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More building blocks:\n",
    "\n",
    "We need to add a few other techniques:\n",
    "\n",
    "![blocks](./images/WaveNet_residblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gated Activations**\n",
    "\n",
    "In the boxed portion of the architecture diagram above, note the dilated convolution splits into two branches that recombine via element-wise multiplication. This is a **gated activation unit**:\n",
    "- **tanh:** activation branch is a learned (model optimized) filter\n",
    "- **sigmoid:** activation branch is a learned gate that controls flow from the *tanh* filter (just like LSTM gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "\n",
    "In fully-connected NNs, a neuron takes inputs from all neurons in the previous layer: early layers establish later ones via a hierarchy of intermediate computations. This allows NNs to build complex interactions of raw inputs/signals.\n",
    "\n",
    "But... what if raw inputs are directly useful for prediction, and we want them to directly influence the output? **Skip connections** allow outputs of any layer to bypass multiple future layers and skip influence dilution! Keras allows us to store the tensor output of each convolutional block - in addition to passing it through further layers - with `skips.append()`. Note how for each block in the stack above, the output from the gated activations joins the set of skip connections. How much or how far you do this is a hyper-parameter that depends on your data and model structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections\n",
    "\n",
    "![CNN_skips](./images/CNN_skips.png)\n",
    "\n",
    "**Residual connections** are similar to skip connections: think of them as consistenly-available short layer skips! We'll use a one-layer skip for our model, but it's also a hyper-parameter. Why they help is mysterious, but it's most likely due to helping with the [Vanishing or Exploding Gradient](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) obstacle in backpropagation. This becomes more important with larger models, but I'll show you the implementation in a smaller setting for educational purposes.\n",
    "\n",
    "> Note: the diagram's *1x1 convolutions* are basically time-distributed fully connected layers; this allows us to use CNN language for our architecting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XKCD](./images/XKCD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup\n",
    "\n",
    "First, Keras needs us to convert our series into `numpy` arrays. As we prep our data, we should have a partition strategy to validate and test our model's results. [Artur Suilin](https://github.com/Arturus/kaggle-web-traffic/blob/master/how_it_works.md) created a visualization of the **walk-forward validation** strategy that serves as a best practice:\n",
    "\n",
    "![walk-forward validation](./images/ArturSuilin_validation.png)\n",
    "\n",
    "Let's use this approach as we parse, learn, and validate our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to fit structure below\n",
    "aapl = aapl[['Volume', 'AC']]\n",
    "aapl = aapl.T\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "data_start_date = aapl.columns[1]\n",
    "data_end_date = aapl.columns[-1]\n",
    "\n",
    "pred_steps = 14\n",
    "pred_length=timedelta(pred_steps)\n",
    "\n",
    "first_day = pd.to_datetime(data_start_date) \n",
    "last_day = pd.to_datetime(data_end_date)\n",
    "\n",
    "val_pred_start = last_day - pred_length + timedelta(1)\n",
    "val_pred_end = last_day\n",
    "\n",
    "train_pred_start = val_pred_start - pred_length\n",
    "train_pred_end = val_pred_start - timedelta(days=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_length = train_pred_start - first_day\n",
    "\n",
    "train_enc_start = first_day\n",
    "train_enc_end = train_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "val_enc_start = train_enc_start + pred_length\n",
    "val_enc_end = val_enc_start + enc_length - timedelta(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train encoding:', train_enc_start, '-', train_enc_end)\n",
    "print('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\n",
    "print('Val encoding:', val_enc_start, '-', val_enc_end)\n",
    "print('Val prediction:', val_pred_start, '-', val_pred_end)\n",
    "\n",
    "print('\\nEncoding interval:', enc_length.days)\n",
    "print('Prediction interval:', pred_length.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in aapl.columns[1:]]),\n",
    "                          data=[i for i in range(len(aapl.columns[1:]))])\n",
    "\n",
    "series_array = aapl[aapl.columns[1:]].values\n",
    "\n",
    "def get_time_block_series(series_array, date_to_index, start_date, end_date):\n",
    "    \n",
    "    inds = date_to_index[start_date:end_date]\n",
    "    return series_array[:,inds]\n",
    "\n",
    "def transform_series_encode(series_array):\n",
    "    \n",
    "    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n",
    "    series_mean = series_array.mean(axis=1).reshape(-1,1) \n",
    "    series_array = series_array - series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array, series_mean\n",
    "\n",
    "def transform_series_decode(series_array, encode_series_mean):\n",
    "    \n",
    "    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n",
    "    series_array = series_array - encode_series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Overview\n",
    "\n",
    "Let's build our model like so:\n",
    "\n",
    "* 16 dilated causal convolutional blocks\n",
    "    * Pre and postprocessing (time distributed) fully connected layers (i.e. convolutions with filter width 1):\n",
    "        * 16 output units\n",
    "    * 32 filters of width 2 per block\n",
    "    * Exponentially increasing dilation rate with a reset (1, 2, 4, 8,... 2^6), two sets\n",
    "    * Gated activations\n",
    "    * Residual and skip connections\n",
    "* 2 fully connected layers to combine skip outputs to final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional operation parameters\n",
    "n_filters = 16 \n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(5)] * 2 \n",
    "\n",
    "# define an input history series and pass it through a stack of dilated causal convolution blocks. \n",
    "history_seq = Input(shape=(None, 1))\n",
    "x = history_seq\n",
    "\n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "    \n",
    "    # preprocessing - equivalent to time-distributed dense\n",
    "    x = Conv1D(16, 1, padding='same', activation='relu')(x) \n",
    "    \n",
    "    # filter convolution\n",
    "    x_f = Conv1D(filters=n_filters,\n",
    "                 kernel_size=filter_width, \n",
    "                 padding='causal',\n",
    "                 dilation_rate=dilation_rate)(x)\n",
    "    \n",
    "    # gating convolution\n",
    "    x_g = Conv1D(filters=n_filters,\n",
    "                 kernel_size=filter_width, \n",
    "                 padding='causal',\n",
    "                 dilation_rate=dilation_rate)(x)\n",
    "    \n",
    "    # multiply filter and gating branches\n",
    "    z = Multiply()([Activation('tanh')(x_f),\n",
    "                    Activation('sigmoid')(x_g)])\n",
    "    \n",
    "    # postprocessing - equivalent to time-distributed dense\n",
    "    z = Conv1D(16, 1, padding='same', activation='relu')(z)\n",
    "    \n",
    "    # residual connection\n",
    "    x = Add()([x, z])    \n",
    "    \n",
    "    # collect skip connections\n",
    "    skips.append(z)\n",
    "\n",
    "# add all skip connection outputs \n",
    "out = Activation('relu')(Add()(skips))\n",
    "\n",
    "# final time-distributed dense layers \n",
    "out = Conv1D(128, 1, padding='same')(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Dropout(.2)(out)\n",
    "out = Conv1D(1, 1, padding='same')(out)\n",
    "\n",
    "# extract the last 60 time steps as the training target\n",
    "def slice(x, seq_length):\n",
    "    return x[:,-seq_length:,:]\n",
    "\n",
    "pred_seq_train = Lambda(slice, arguments={'seq_length':10})(out)\n",
    "\n",
    "model = Model(history_seq, pred_seq_train)\n",
    "model.compile(Adam(), loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print our model architecture\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "first_n_samples = 2\n",
    "\n",
    "# sample of series from train_enc_start to train_enc_end  \n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, \n",
    "                                           train_enc_start, train_enc_end)[:first_n_samples]\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "# sample of series from train_pred_start to train_pred_end \n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, \n",
    "                                            train_pred_start, train_pred_end)[:first_n_samples]\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n",
    "\n",
    "# we append a lagged history of the target series to the input data, \n",
    "# so that we can train with teacher forcing\n",
    "lagged_target_history = decoder_target_data[:,:-1,:1]\n",
    "\n",
    "encoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(), loss='mean_absolute_error')\n",
    "\n",
    "history = model.fit(encoder_input_data, decoder_target_data,\n",
    "                    batch_size=2**6,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Validation']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "Many thanks to my amazing colleague [Joe Eddy](https://www.linkedin.com/in/joseph-eddy-178425129/) for his informative blog posts on WaveNet:\n",
    "- [Time Series Forecasting with Convolutional Neural Networks - a Look at WaveNet](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/)\n",
    "- [Time Series Forecasting with Convolutional Neural Networks - Further Exploration of WaveNet](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv2/)\n",
    "- Optional context: [Forecasting with Neural Networks - An Introduction to Sequence-to-Sequence Modeling Of Time Series](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/)\n",
    "\n",
    "\n",
    "## WaveNet Official\n",
    "- [Blog on audio applications](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n",
    "- [Voice showcase](https://cloud.google.com/text-to-speech/docs/wavenet)\n",
    "- [Paper](https://arxiv.org/abs/1609.03499)\n",
    "- [Wiki](https://en.wikipedia.org/wiki/WaveNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "384px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
